 # F1 FAQ Engine ğŸğŸš¥ğŸ†ğŸï¸ğŸ’¨ â€” GenAI RAG System
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-yellow?style=for-the-badge)](https://huggingface.co/spaces)
[![RAG](https://img.shields.io/badge/RAG-Retrieval%20Augmented%20Generation-blueviolet?style=for-the-badge)](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation)
[![GenAI](https://img.shields.io/badge/GenAI-Powered-FF6F00?style=for-the-badge&logo=openai&logoColor=white)](https://www.anthropic.com/)
[![Formula 1](https://img.shields.io/badge/Formula%201-E10600?style=for-the-badge&logo=f1&logoColor=white)](https://www.formula1.com/)

**What it does**  
F1 FAQ Engine lets you ask natural language questions about Formula 1 race results through history (1950â€“2024) and get grounded answers generated by an LLM using Hybrid (vector + keyword) retrieval from Weaviate. Try it out my Formula 1 Race History AI Assistant!

## ğŸš€ Demo  
ğŸ”— Live demo: *https://huggingface.co/spaces/jaimegalanmartinez/f1_faq_engine*

## ğŸ§  Motivation  
Instead of static lookups or manual queries, this system combines retrieval and generative models to provide precise, contextual answers from structured F1 data.

## ğŸ— Architecture
- Weaviate stores  lgenerated local embeddings vectors + structured metadata  
- Hybrid retrieval (semantic + keyword) improves precision  
- Openâ€‘source LLM provides response generation

## ğŸ’¡ Features
- RAG Hybrid retrieval (vector + keyword) with structured filters (year, race, driver, constructor, circuit)
- Grounded answers with concise context
- Multilingual-friendly open source embeddings (BGE-M3 from Beijing Academy of AI) (https://huggingface.co/BAAI/bge-m3)
- Open source LLM for answering questions: katanemo/Arch-Router-1.5B:hf-inference (https://huggingface.co/katanemo/Arch-Router-1.5B)

## ğŸ›  Tech Stack
- Python 3.12  
- Weaviate (vector database)  
- BGEâ€‘M3 embeddings  
- HuggingFace LLM inference  
- Gradio UI app

**Data Sources**
- Kaggle â€œFormula 1 World Championship (1950â€“2024)â€ by Rohan Rao

**Limitations And Future Work**
- 2025 season results are not included yet (planned append step).
- Driver and constructor standings are out of scope for the current demo.

## Running it locally with Docker

### Prerequisites
- Docker installed
- Weaviate Cloud account with a cluster set up
- `.env` file with your credentials (see below)
- Run ingest_data.py to create your Weaviate vector DB collection and fill it with F1 data.

### Setup

1. **Create a `.env` file** in the project root (do NOT use quotes):
```bash
WEAVIATE_URL=https://your-cluster.weaviate.network
WEAVIATE_API_KEY=your-api-key-here
HF_TOKEN=your HuggingFace token
```

2. **Build the Docker image**:
```bash
docker build -t f1_faq_engine .
```

3. **Run the container**:
```bash
docker run -p 7860:7860 --env-file .env f1_faq_engine
```

4. **Access the application**:
Open your browser and navigate to interact with your own Formula 1 Race History Assistant `http://localhost:7860`
